\documentclass{article} % For LaTeX2e
\usepackage{cos424,times}
\usepackage{url}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{amsmath}

\usepackage{biblatex}
\bibliography{bib.bib}

\title{Already Asked: Detecting Duplicate Question Pairs in the Quora Dataset}


\author{
Meir Hirsch \\
Computer Science\\
\texttt{ehirsch@} \\
\And
Charles Stahl \\
Physics \\
\texttt{cnstahl@} \\
\And
Kenan Farmer\\
Computer Science \\
\texttt{kfarmer@}\\
}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}
\newcommand{\wordtvec}{\texttt{word2vec}}

\begin{document}

\maketitle

\begin{abstract}
This project uses a dataset posted on Kaggle~\cite{kaggleComp} to detect duplicate question pairs on Quora, a popular question-and-answer website~\cite{quora} that received 100 million monthly unique visitors as of March 2016~\cite{qvisit}. The training data includes 404,290 question pairs chosen from 537,933 questions, classified as duplicates or not. The testing set was 2.3 million unlabeled pairs, with 4.2 million unique questions. We used bag-of-words (BOW) methods, \wordtvec, and word nets to reduce the dimensionality of the data, and \_\_methods\_\_ to classify duplicates. The best results were obtained by \_\_\_\_.
\end{abstract}

\section{Introduction}

Quora is a website that strives to connect people from different backgrounds to be able to answer questions whose answers are "either locked in peopleâ€™s heads, or only accessible to select groups" \cite{quora}. Questions range from "What is the most embarrassing text message you have sent to the wrong person?" to "How do I identify entities in natural language search query?" to "Do virtual particles and energy in vacuum really exist?"

Once these questions are answered, any other person should be able to find the responses, and the questions never needs to be asked again. However, sometimes people are unable to find the questions they want and end up asking the same question again. This counteracts Quora's vision of having "only one version of each question\dots [not] a left wing version, a right wing version, a western version, and an eastern version"~\cite{quora}. Duplicate questions also waste resources for the website, the answerers, and future viewers who will either see one answer or the other, but not the full response the website has to offer.

Quora currently identifies duplicate questions with a Random Forest model~\cite{kaggleComp}. Quora does not have a method for users to mark questions as duplicates, although they allow users to redirect questions. Stack Exchange, a similar question-answer website, does allow users to mark questions as duplicates~\cite{stackdup}. 

To improve their duplicate identification, Quora released a dataset of question pairs on Kaggle, a platform for data science-based prediction and classification problems~\cite{kaggleComp}.



\section{Related Work}

Zhang, et. al. created a duplicate detection tool for use on the Stack Overflow (SO), a website in the Stack Exchange network~\cite{Zhang2015}. It uses the title, content, and tags of the questions to assign topics using Latent Dirichlet Allocation (LDA)~\cite{Blei03}. The method then predicts duplicates by comparing topic distributions.

Since Quora questions include only the single sentence question, they are nearly equivalent to just the title of an SO question. The tags and content on SO are more significant and better indicate the meaning of a question, meaning this method is not useful on the Quora dataset. This analysis also only used bag-of-words text similarity and LDA to detect duplicates.

An earlier paper focused on detecting duplicate bug reports~\cite{Runeson2007}. The motivation of this problem is similar, centering on the wasted time used to identify and respond to duplicate reports. This study used standard NLP tools along with a term-frequency vector space to define distance measured between reports. 

\section{The Data}

As mentioned in the abstract, the testing set is significantly larger than the training set. Each line of the training set consists of a pair ID, two question IDs, the text of two questions, and an indication of whether the questions have the same intent. The questions are usually between 7 and 12 words in length.

Since the text of the testing data would be available in a real-world application of this problem, we decided that training models on this text would not be considered double-dipping, especially since we did not have labels for this text. 

\section{Methods}

Should we tf-idf?

\subsection{Feature Engineering} \label{sub:features}

Our na\"ive method was a BOW method, but instead of defining an individual bag for each question, we defined a bag for each pair of questions, only including words unique to one question or the other. A simply extension of this method 

A major source of features was word embeddings, using Gensim's implementation of \wordtvec~\cite{gensim}. Word embeddings were introduced in the early 2000's to address the high rate of orthogonality inherent in BOW-based methods~\cite{Bengio03}. Since the BOW methods only look at counts of words, the meanings of the words are not encoded.

A canonical example is from~\cite{kusner15}. Consider the two sentences
\begin{center}
\text{Obama speaks to the media in Illinois.}
\end{center}
and 
\begin{center}
\text{The President greets the press in Chicago}.
\end{center}
These sentences are orthogonal in a space that assigns each word its own dimension. Therefore they have a cosine similarity of 0. However, an effective similarity measure should indicate that these sentences are very closely related.

Word embedding looks at a large corpus to find how different words are related and connected. It creates a space with dimension lower than the number of words, so that each word is represented as a non-sparse vector. Words with similar meaning are closer in this space, giving a high similarity score for the above sentences. Furthermore, algebraic manipulation of these vectors carries significance. Ideally this would lead to usable analogies, such as \textit{Paris - France + Italy = Rome}, or \textit{suhi - Japan + Germany = bratwurst}.

The 2013 implementation from Google, called \wordtvec, uses a shallow neural network to embed words in the vector space~\cite{word2vec}. For a detailed description of \wordtvec\ see subsection\ref{sub:detail}.

From the \wordtvec\ embeddings of the words in each question, we can compute the word mover's distance (WMD)~\cite{kusner15}. For sentences $M$ and $N$ of length $J$ and $K$ with words $m^j$ and $n^k$, the WMD is defined as 
\begin{align}
\text{WMD}(M,N) = \sum_{j=1}^J\min_{n^k\in N}\left|m^j-n^k\right|,
\end{align}
the sum of the distances from each word in $M$ to the nearest word in $N$.

WordNet~\cite{wordnet}.

\subsection{Methodology}

The training data was read from csv files using Pandas~\cite{pandas}. Lemmatization and stop word removal was completed using the Spacy package, a package for "Industrial-Strength Natural Language Processing"~\cite{spacy}. 

\newpage
\subsection{One method in detail: \wordtvec} \label{sub:detail}

The \wordtvec\ method introduced in subsection~\ref{sub:features} merits further discussion. Training the model uses a 2-level neural network. The implementation we used was hidden, but understanding the working is still useful. The presentation here is based on a tutorial from Tensorflow~\cite{tensorflow}.

As previously mentioned, the motivation of word embeddings is to find a vector representation for words that is not as sparse as BOW-based methods. 

\section{Results}

\section{Discussion and Conclusion}

\subsubsection*{Acknowledgments}


\printbibliography

\end{document}
