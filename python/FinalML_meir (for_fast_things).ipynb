{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "### Import precomp libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en') # this should take some time like 10s to load\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the spacy english model\n",
    "# \"python -m spacy download en\"   to download english models\n",
    "\n",
    "import pickle \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 6) (2345796, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/ipykernel_launcher.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# PROJECT structure: data directory with the two csvs of train,test and the ipython notbook in its own directory\n",
    "# loading the data\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "print train.shape, test.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test.loc[379205]['question2'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "def stem():\n",
    "    porter = PorterStemmer()\n",
    "    for w in l:\n",
    "        porter.stem(w)\n",
    "stem('What are the stories of Kohinoor (Koh-i-Noor) Diamond? men walked to the stores to buy shirts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse(sentence):\n",
    "    bow = []\n",
    "    sentence = sentence.decode('utf-8')\n",
    "    p = nlp(sentence)\n",
    "    for word in p:\n",
    "        if word.is_stop: continue\n",
    "        bow.append(word.lemma_)\n",
    "    return Counter(bow)\n",
    "        \n",
    "parse('What are the stories of Kohinoor (Koh-i-Noor) Diamond? men walked to the stores to buy shirts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_vecs(corpus):\n",
    "    vect_list = []\n",
    "    results = []\n",
    "    n = len(train)\n",
    "    for i in range(n): \n",
    "        if i > 10: return \n",
    "        f = train.loc[i]\n",
    "        q1, q2 = f['question1'], f['question2']\n",
    "        first = parse(q1)\n",
    "        first.subtract(parse(q2))\n",
    "        print first\n",
    "print_vecs(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_vecs(corpus, results=False):\n",
    "    vect_list = []\n",
    "    results = []\n",
    "    n = len(corpus)\n",
    "    for i in range(n): \n",
    "        if i % 10000 == 0: print i\n",
    "        f = corpus.loc[i]\n",
    "        q1, q2 = f['question1'], f['question2']\n",
    "        try: \n",
    "            p1 = parse(q1)\n",
    "            p2 = parse(q2)\n",
    "            diff1 = p1 - p2\n",
    "            diff2 = p2 - p1\n",
    "            vect_list.append((diff1, diff2))\n",
    "            if results: results.append(f['is_duplicate'])\n",
    "        except Exception as e:\n",
    "            vect_list.append('')\n",
    "            print(i, e)\n",
    "    return fv, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv_train, target = make_vecs(train, results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fv_test, results = make_vecs(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('fv_train','w') as f:\n",
    "#     pickle.dump(fv_train_,f)\n",
    "# with open('target','w') as f:\n",
    "#     pickle.dump(target,f)\n",
    "# with open('fv_test','w') as f:\n",
    "#     pickle.dump(fv_test,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fv_train_p = pickle.load(open('fv_train', 'rb'))\n",
    "# fv_test_p = pickle.load(open('fv_test', 'rb'))\n",
    "target_p = pickle.load(open('target', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fv_train_p[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(vectors):\n",
    "    v1, v2 = zip(*vectors)\n",
    "    vectorizer = DictVectorizer()\n",
    "    x1 = vectorizer.fit_transform(v1)\n",
    "    x2 = vectorizer.fit_transform(v2)\n",
    "    X = hstack([x1,x2])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorize(fv_train_p)\n",
    "target = target_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "clfs = [SVC(kernel='linear', probability=True)] \n",
    "#[MultinomialNB(), LogisticRegression()]\n",
    "\n",
    "for clf in clfs:\n",
    "    print clf\n",
    "    scores = cross_val_score(clf, X, target, cv=3, scoring='neg_log_loss')\n",
    "    print \"Log Loss: %0.5f\" % scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('submission', 'rb'))\n",
    "f.write('test_id,is_duplicate')\n",
    "fv_test = vectorize(fv_test_p)\n",
    "clf = MultinomialNB().fit(X, target)\n",
    "predict_proba(fv_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this procedure will be mapped to each row\n",
    "def eval_sysim(row):\n",
    "    q1_txt = nlp(unicode(row['question1']))\n",
    "    q2_txt = nlp(unicode(row['question2']))\n",
    "\n",
    "    # feature: similarity measure from built-in spacy\n",
    "    #word net similarity? or hamming distance of the strings?\n",
    "    sy_sim = q1_txt.similarity(q2_txt)\n",
    "\n",
    "    #feature: \n",
    "    return sy_sim\n",
    "    \n",
    "#unit test(s)\n",
    "utest = train.loc[1]\n",
    "print eval_sysim(utest)\n",
    "utest = train.loc[3]\n",
    "print eval_sysim(utest)\n",
    "utest = train.loc[9]\n",
    "print eval_sysim(utest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we see the pitfall in relying on the similarity measure of the two doccuments as their scope differs(invest in shares vs invest in shares of **INDIA**). To extend this baseline model we will look at parts of speech and entity recognition to help parse scope.\n",
    "\n",
    "We can however use the similarity as the first filter to remove completely unrelated questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
