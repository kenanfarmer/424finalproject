{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Investigation of Similarity measures\n",
    "### Import precomp libs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import the spacy english model\n",
    "# \"python -m spacy download en\"   to download english models\n",
    "import spacy\n",
    "nlp = spacy.load('en') # this should take some time like 10s to load\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 6) (2345796, 3)\n"
     ]
    }
   ],
   "source": [
    "# PROJECT structure: data directory with the two csvs of train,test and the ipython notbook in its own directory\n",
    "# loading the data\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "print train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "5   5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6   6    13    14                                Should I buy tiago?   \n",
       "7   7    15    16                     How can I be a good geologist?   \n",
       "8   8    17    18                    When do you use シ instead of し?   \n",
       "9   9    19    20  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "6  What keeps childern active and far from phone ...             0  \n",
       "7          What should I do to be a great geologist?             1  \n",
       "8              When do you use \"&\" instead of \"and\"?             0  \n",
       "9  How do I hack Motorola DCX3400 for free internet?             0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.934445645664\n",
      "0.740894391357\n",
      "0.819441244603\n"
     ]
    }
   ],
   "source": [
    "#this procedure will be mapped to each row\n",
    "def eval_sysim(row):\n",
    "    q1_txt = nlp(unicode(row['question1']))\n",
    "    q2_txt = nlp(unicode(row['question2']))\n",
    "    # feature: similarity measure from built-in spacy\n",
    "    #word net similarity? or hamming distance of the strings?\n",
    "    sy_sim = q1_txt.similarity(q2_txt)\n",
    "\n",
    "    #feature: \n",
    "    return sy_sim\n",
    "\n",
    "def u_eval_test(fun):\n",
    "    #unit test(s)\n",
    "    utest = train.loc[1]\n",
    "    print fun(utest)\n",
    "    utest = train.loc[3]\n",
    "    print fun(utest)\n",
    "    utest = train.loc[9]\n",
    "    print fun(utest)\n",
    "    \n",
    "\n",
    "\n",
    "def eval_sysim1(row):\n",
    "    q1_txt = nlp(unicode(row['question1']))\n",
    "    q2_txt = nlp(unicode(row['question2']))\n",
    "    q1 = []\n",
    "    q2 = []\n",
    "    for w1 in q1_txt:\n",
    "        if (w1.is_stop == False):\n",
    "            q1.append(w1)\n",
    "    for w2 in q2_txt:\n",
    "        if (w2.is_stop == False):\n",
    "            q2.append(w1)\n",
    "    # feature: similarity measure from built-in spacy\n",
    "    #word net similarity? or hamming distance of the strings?\n",
    "    q1 = nlp(q1)\n",
    "    q2 = nlp(q2)\n",
    "    sim = 0.0\n",
    "    \n",
    "    reduce(lambda x,y: x+y, )\n",
    "    sy_sim = q1.similarity(q2)\n",
    "\n",
    "    #feature: \n",
    "    return sy_sim\n",
    "\n",
    "u_eval_test(eval_sysim)\n",
    "#u_eval_test(eval_sysim1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.181818181818\n"
     ]
    }
   ],
   "source": [
    "def jacc_sim(row):\n",
    "    q1_txt = nlp(unicode(row['question1']))\n",
    "    q2_txt = nlp(unicode(row['question2']))\n",
    "    #test jacc sim\n",
    "    q1 = set()\n",
    "    q2 = set()\n",
    "    for w in q1_txt:\n",
    "        if w.pos_ in ['NOUN','PROPN','VERB']:\n",
    "            q1.add((w.text, w.pos_))\n",
    "    for w in q2_txt:\n",
    "        if w.pos_ in ['NOUN','PROPN','VERB']:\n",
    "            q2.add((w.text, w.pos_))\n",
    "    return len(q1.intersection(q2)) / (1.0*len(q1.union(q2)))   \n",
    "\n",
    "print jacc_sim(train.loc[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3205414059688226"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "#IMPORTANCE OF ENTITIES NOT IN BOTH\n",
    "diff_ents = []\n",
    "def diff_ents(row):\n",
    "    q1_txt = nlp(unicode(row['question1']))\n",
    "    q2_txt = nlp(unicode(row['question2']))\n",
    "    #test jacc sim\n",
    "    q1 = set()\n",
    "    q2 = set()\n",
    "    q1_n = []\n",
    "    q2_n = []\n",
    "    for w in q1_txt.ents:\n",
    "        q1.add(w)\n",
    "    for w in q2_txt.ents:\n",
    "        q2.add(w)\n",
    "    for n in q1_txt.noun_chunks:\n",
    "        q1_n.append(n)\n",
    "    for n in q2_txt.noun_chunks:\n",
    "        q2_n.append(n)    \n",
    "    return (q1,q2,q1_n,q2_n) \n",
    "\n",
    "a,b,c,d = diff_ents(train.loc[5])\n",
    "c = map(lambda x: x.text.split(), c)\n",
    "c = [item for sublist in c for item in sublist]\n",
    "d = map(lambda x: x.text.split(), d)\n",
    "d = [item for sublist in d for item in sublist]\n",
    "rez = []\n",
    "for word1 in c:\n",
    "    for word2 in d:\n",
    "        wordFromList1 = wn.synsets(word1)\n",
    "        wordFromList2 = wn.synsets(word2)\n",
    "        if wordFromList1 and wordFromList2: #Thanks to @alexis' note\n",
    "            s = wordFromList1[0].wup_similarity(wordFromList2[0])\n",
    "            rez.append(s)\n",
    "reduce(lambda x,y: x+y, rez)/len(rez)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "here we see the pitfall in relying on the similarity measure of the two doccuments as their scope differs(invest in shares vs invest in shares of **INDIA**). To extend this baseline model we will look at parts of speech and entity recognition to help parse scope.\n",
    "\n",
    "We can however use the similarity as the first filter to remove completely unrelated questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.367406582388\n",
      "0.388970049558\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def eval_wordnet(row):\n",
    "    q1_txt = row['question1'].split()\n",
    "    q2_txt = row['question2'].split()\n",
    "    \n",
    "    #ADD TOKENIZATION\n",
    "    \n",
    "    rez=[]\n",
    "    \n",
    "    #this is word against every other word\n",
    "    for word1 in q1_txt:\n",
    "        for word2 in q2_txt:\n",
    "            wordFromList1 = wn.synsets(word1)\n",
    "            wordFromList2 = wn.synsets(word2)\n",
    "            if wordFromList1 and wordFromList2: #Thanks to @alexis' note\n",
    "                s = wordFromList1[0].wup_similarity(wordFromList2[0])\n",
    "                rez.append(s)\n",
    "    return reduce(lambda x,y: x+y, rez)/len(rez)\n",
    "\n",
    "utest = train.loc[9]\n",
    "print eval_wordnet(utest)\n",
    "\n",
    "utest = train.loc[7]\n",
    "print eval_wordnet(utest)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "objts = []\n",
    "\n",
    "def parse_clean(row):\n",
    "    q1_txt = [word for word in row['question1'].split() if word not in stopwords.words('english')]\n",
    "    q2_txt = [word for word in row['question2'].split() if word not in stopwords.words('english')]\n",
    "    return (q1_txt,q2_txt)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(sent):\n",
    "    tokens = word_tokenize(sent)\n",
    "    return tokens\n",
    "\n",
    "def parse_clean(row):\n",
    "    q1_txt = [word for word in row['question1'].split() if word not in stopwords.words('english')]\n",
    "    q2_txt = [word for word in row['question2'].split() if word not in stopwords.words('english')]\n",
    "    return (q1_txt,q2_txt)\n",
    "\n",
    "import nltk\n",
    "def pos_tag(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def stem(tokens):\n",
    "    pstem = PorterStemmer()\n",
    "    return map(lambda x: pstem.stem(x),tokens)\n",
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def word_stem(tokens):\n",
    "    rez = []\n",
    "    for tok in tokens:\n",
    "        tokdk = nlp(unicode(tok))\n",
    "        pos = \"\"\n",
    "        for x in tokdk:\n",
    "            pos = x.pos_\n",
    "        pstem = WordNetLemmatizer()\n",
    "        print pos\n",
    "        rez.append(pstem.lemmatize(tok))\n",
    "    return rez \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I will investigate regression on the POS tagging and have one feature be lemmatized verb similarity and entity similarites with penalty for entities not encluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# append all text of the questions together for training\n",
    "texts = map(lambda x:  str(x), train.loc[:,'question1']) \\\n",
    "+ map(lambda x:  str(x), train.loc[:,'question2']) \\\n",
    "+ map(lambda x: str(x), test.loc[:,'question1']) \\\n",
    "+ map(lambda x: str(x), test.loc[:,'question2'])\n",
    "\n",
    "# delete duplicates\n",
    "qs = set(texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "qs = [x for x in qs if x is not None]\n",
    "\n",
    "lemmas = []\n",
    "for idx, doc in enumerate(docs):\n",
    "    lemma = []\n",
    "    for word in doc:\n",
    "        if not word.is_punct: lemma.append(word.lemma_)\n",
    "    lemmas.append(lemma)\n",
    "    if idx == 20: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "# PRE-PROCESS THE SENTENCES AND STEM / LEMMATIZE!!!!\n",
    "def pipe_clean(sent):\n",
    "    \n",
    "sentz = [pipe_clean(x) for x in qs]\n",
    "#train our word to vec model and get ready\n",
    "model = word2vec.Word2Vec(sentz, min_count=1)\n",
    "model.save(\"saved_vec_mod.kf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"saved_vec_mod.kf\")\n",
    "\n",
    "def vec_diff(doc1,doc2):\n",
    "    a = doc1.vector\n",
    "    b = doc2.vector\n",
    "    return a - b\n",
    "    \n",
    "def wmdist(sent1,sent2):\n",
    "    \"\"\"This requires that the parameters @sent1 @sent2 are lists of strings ideally\n",
    "    lemmatized and cleaned sentences from above\"\"\"\n",
    "    return model.wmdistance(sent1, sent2)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_diff(nlp(unicode(list(qs)[2])), nlp(unicode(list(qs)[4])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.174199494647931"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmdist(texts[7],texts[train.shape[0]+7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-c1c2631546e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mlemma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_punct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spacy/language.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, tag, parse, entity)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mproc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 329\u001b[0;31m                 \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    330\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spacy/syntax/parser.pyx\u001b[0m in \u001b[0;36mspacy.syntax.parser.Parser.__call__ (spacy/syntax/parser.cpp:7941)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mstatus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparseC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnr_feat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0;31m# Check for KeyboardInterrupt etc. Untested\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0mPyErr_CheckSignals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mParserStateError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "lemmas = []\n",
    "for idx, q in enumerate(docs):\n",
    "    lemma = []\n",
    "    doc = nlp(q.decode('utf-8'))\n",
    "    for word in doc:\n",
    "        if not word.is_punct:\n",
    "            if word.lemma_ == '-PRON-':\n",
    "                lemma.append(unicode(word))\n",
    "            else:\n",
    "                lemma.append(word.lemma_)\n",
    "    lemmas.append(lemma)\n",
    "print len(lemmas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "READ THE DATA and then train model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_loc = 'test_dump.bin'\n",
    "train_loc = 'train_dump.bin'\n",
    "\n",
    "from spacy.tokens.doc import Doc\n",
    "test_docs = []\n",
    "train_docs = []\n",
    "i = 0\n",
    "#print datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "with open(train_loc, 'rb') as file_:\n",
    "    for byte_string in Doc.read_bytes(file_):\n",
    "        #if i%10000 == 0: print i, datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        train_docs.append(Doc(nlp.vocab).from_bytes(byte_string))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2017-05-13 20:15:25\n",
      "100000 2017-05-13 20:15:35\n",
      "200000 2017-05-13 20:15:44\n",
      "300000 2017-05-13 20:15:54\n",
      "400000 2017-05-13 20:16:03\n",
      "500000 2017-05-13 20:16:12\n",
      "600000 2017-05-13 20:16:22\n",
      "700000 2017-05-13 20:16:31\n",
      "800000 2017-05-13 20:16:41\n",
      "900000 2017-05-13 20:16:52\n",
      "1000000 2017-05-13 20:17:01\n",
      "1100000 2017-05-13 20:17:10\n",
      "1200000 2017-05-13 20:17:19\n",
      "1300000 2017-05-13 20:17:38\n",
      "1400000 2017-05-13 20:17:48\n",
      "1500000 2017-05-13 20:17:57\n",
      "1600000 2017-05-13 20:18:06\n",
      "1700000 2017-05-13 20:18:15\n",
      "1800000 2017-05-13 20:18:24\n",
      "1900000 2017-05-13 20:18:54\n",
      "2000000 2017-05-13 20:19:03\n",
      "2100000 2017-05-13 20:19:12\n",
      "2200000 2017-05-13 20:19:22\n",
      "2300000 2017-05-13 20:19:31\n",
      "2400000 2017-05-13 20:19:40\n",
      "2500000 2017-05-13 20:20:18\n",
      "2600000 2017-05-13 20:20:27\n",
      "2700000 2017-05-13 20:20:36\n",
      "2800000 2017-05-13 20:20:45\n",
      "2900000 2017-05-13 20:20:54\n",
      "3000000 2017-05-13 20:21:04\n",
      "3100000 2017-05-13 20:21:12\n",
      "3200000 2017-05-13 20:21:22\n",
      "3300000 2017-05-13 20:21:32\n",
      "3400000 2017-05-13 20:22:24\n",
      "3500000 2017-05-13 20:22:36\n",
      "3600000 2017-05-13 20:22:47\n",
      "3700000 2017-05-13 20:22:57\n",
      "3800000 2017-05-13 20:23:07\n",
      "3900000 2017-05-13 20:23:18\n",
      "4000000 2017-05-13 20:23:28\n",
      "4100000 2017-05-13 20:23:38\n",
      "4200000 2017-05-13 20:23:48\n",
      "4300000 2017-05-13 20:23:58\n",
      "4400000 2017-05-13 20:25:02\n",
      "4500000 2017-05-13 20:25:13\n",
      "4600000 2017-05-13 20:25:25\n"
     ]
    }
   ],
   "source": [
    "i=0\n",
    "with open(test_loc, 'rb') as file_:\n",
    "    for byte_string in Doc.read_bytes(file_):\n",
    "        if i%100000 == 0: print i, datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        test_docs.append(Doc(nlp.vocab).from_bytes(byte_string))\n",
    "        i += 1\n",
    "        \n",
    "#         if i == 10: break\n",
    "#print datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "docs = train_docs + test_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000380992889404\n",
      "12.0580840111\n",
      "18.9055030346\n",
      "84.5726351738\n",
      "94.4663629532\n",
      "103.188327074\n",
      "110.556419134\n",
      "116.528983116\n",
      "205.783671141\n",
      "217.097140074\n",
      "233.241887093\n",
      "275.003391027\n",
      "288.963165998\n",
      "303.790642977\n",
      "407.842146158\n",
      "429.021991968\n",
      "442.510020971\n",
      "453.314794064\n",
      "462.367565155\n",
      "468.373139143\n",
      "480.398862123\n",
      "613.231790066\n",
      "641.839128017\n",
      "653.091371059\n",
      "660.687510967\n",
      "666.921568155\n",
      "686.481702089\n",
      "697.070707083\n",
      "723.861688137\n",
      "732.505973101\n",
      "739.9001441\n",
      "1002.07970905\n",
      "1015.29264998\n",
      "1033.14704204\n",
      "1045.67611814\n",
      "1059.63146114\n",
      "1091.12354517\n",
      "1139.30702496\n",
      "1164.7583971\n",
      "1175.28036094\n",
      "1183.44532204\n",
      "1193.69142008\n"
     ]
    }
   ],
   "source": [
    "lemmas = []\n",
    "st_tm = time.time()\n",
    "for idx, doc in enumerate(docs):\n",
    "    lemma = []\n",
    "    for word in doc:\n",
    "        if not word.is_punct:\n",
    "            if word.lemma_ == '-PRON-':\n",
    "                lemma.append(unicode(word))\n",
    "            else:\n",
    "                lemma.append(word.lemma_)\n",
    "    lemmas.append(lemma)\n",
    "    if idx % 100000 == 0:\n",
    "        print time.time() - st_tm\n",
    "print lemmas[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"a\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
