{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Investigation of Similarity measures\n",
    "### Import precomp libs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#import the spacy english model\n",
    "# \"python -m spacy download en\"   to download english models\n",
    "import spacy\n",
    "nlp = spacy.load('en') # this should take some time like 10s to load\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "SKIP to the block of reading in SpaCy objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 6) (2345796, 3)\n"
     ]
    }
   ],
   "source": [
    "# PROJECT structure: data directory with the two csvs of train,test and the ipython notbook in its own directory\n",
    "# loading the data\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "print train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>12</td>\n",
       "      <td>Astrology: I am a Capricorn Sun Cap moon and c...</td>\n",
       "      <td>I'm a triple Capricorn (Sun, Moon and ascendan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>Should I buy tiago?</td>\n",
       "      <td>What keeps childern active and far from phone ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>15</td>\n",
       "      <td>16</td>\n",
       "      <td>How can I be a good geologist?</td>\n",
       "      <td>What should I do to be a great geologist?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>When do you use シ instead of し?</td>\n",
       "      <td>When do you use \"&amp;\" instead of \"and\"?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>19</td>\n",
       "      <td>20</td>\n",
       "      <td>Motorola (company): Can I hack my Charter Moto...</td>\n",
       "      <td>How do I hack Motorola DCX3400 for free internet?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "5   5    11    12  Astrology: I am a Capricorn Sun Cap moon and c...   \n",
       "6   6    13    14                                Should I buy tiago?   \n",
       "7   7    15    16                     How can I be a good geologist?   \n",
       "8   8    17    18                    When do you use シ instead of し?   \n",
       "9   9    19    20  Motorola (company): Can I hack my Charter Moto...   \n",
       "\n",
       "                                           question2  is_duplicate  \n",
       "0  What is the step by step guide to invest in sh...             0  \n",
       "1  What would happen if the Indian government sto...             0  \n",
       "2  How can Internet speed be increased by hacking...             0  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0  \n",
       "4            Which fish would survive in salt water?             0  \n",
       "5  I'm a triple Capricorn (Sun, Moon and ascendan...             1  \n",
       "6  What keeps childern active and far from phone ...             0  \n",
       "7          What should I do to be a great geologist?             1  \n",
       "8              When do you use \"&\" instead of \"and\"?             0  \n",
       "9  How do I hack Motorola DCX3400 for free internet?             0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.934445645664\n",
      "0.740894391357\n",
      "0.819441244603\n"
     ]
    }
   ],
   "source": [
    "#this procedure will be mapped to each row\n",
    "def eval_sysim(row):\n",
    "    q1_txt = nlp(unicode(row['question1']))\n",
    "    q2_txt = nlp(unicode(row['question2']))\n",
    "    # feature: similarity measure from built-in spacy\n",
    "    #word net similarity? or hamming distance of the strings?\n",
    "    sy_sim = q1_txt.similarity(q2_txt)\n",
    "\n",
    "    #feature: \n",
    "    return sy_sim\n",
    "\n",
    "def u_eval_test(fun):\n",
    "    #unit test(s)\n",
    "    utest = train.loc[1]\n",
    "    print fun(utest)\n",
    "    utest = train.loc[3]\n",
    "    print fun(utest)\n",
    "    utest = train.loc[9]\n",
    "    print fun(utest)\n",
    "    \n",
    "\n",
    "\n",
    "def eval_sysim1(row):\n",
    "    q1_txt = nlp(unicode(row['question1']))\n",
    "    q2_txt = nlp(unicode(row['question2']))\n",
    "    q1 = []\n",
    "    q2 = []\n",
    "    for w1 in q1_txt:\n",
    "        if (w1.is_stop == False):\n",
    "            q1.append(w1)\n",
    "    for w2 in q2_txt:\n",
    "        if (w2.is_stop == False):\n",
    "            q2.append(w1)\n",
    "    # feature: similarity measure from built-in spacy\n",
    "    #word net similarity? or hamming distance of the strings?\n",
    "    q1 = nlp(q1)\n",
    "    q2 = nlp(q2)\n",
    "    sim = 0.0\n",
    "    \n",
    "    reduce(lambda x,y: x+y, )\n",
    "    sy_sim = q1.similarity(q2)\n",
    "\n",
    "    #feature: \n",
    "    return sy_sim\n",
    "\n",
    "u_eval_test(eval_sysim)\n",
    "#u_eval_test(eval_sysim1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.333333333333\n"
     ]
    }
   ],
   "source": [
    "def jacc_sim(row):\n",
    "    q1_txt = nlp(unicode(row['question1']))\n",
    "    q2_txt = nlp(unicode(row['question2']))\n",
    "    #test jacc sim\n",
    "    q1 = set()\n",
    "    q2 = set()\n",
    "    for w in q1_txt:\n",
    "        if w.pos_ in ['NOUN','PROPN','VERB']:\n",
    "            q1.add((w.text, w.pos_))\n",
    "    for w in q2_txt:\n",
    "        if w.pos_ in ['NOUN','PROPN','VERB']:\n",
    "            q2.add((w.text, w.pos_))\n",
    "    return len(q1.intersection(q2)) / (1.0*len(q1.union(q2)))   \n",
    "\n",
    "print jacc_sim(train.loc[7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3205414059688226"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "#IMPORTANCE OF ENTITIES NOT IN BOTH\n",
    "diff_ents = []\n",
    "def diff_ents(row):\n",
    "    q1_txt = nlp(unicode(row['question1']))\n",
    "    q2_txt = nlp(unicode(row['question2']))\n",
    "    #test jacc sim\n",
    "    q1 = set()\n",
    "    q2 = set()\n",
    "    q1_n = []\n",
    "    q2_n = []\n",
    "    for w in q1_txt.ents:\n",
    "        q1.add(w)\n",
    "    for w in q2_txt.ents:\n",
    "        q2.add(w)\n",
    "    for n in q1_txt.noun_chunks:\n",
    "        q1_n.append(n)\n",
    "    for n in q2_txt.noun_chunks:\n",
    "        q2_n.append(n)    \n",
    "    return (q1,q2,q1_n,q2_n) \n",
    "\n",
    "a,b,c,d = diff_ents(train.loc[5])\n",
    "c = map(lambda x: x.text.split(), c)\n",
    "c = [item for sublist in c for item in sublist]\n",
    "d = map(lambda x: x.text.split(), d)\n",
    "d = [item for sublist in d for item in sublist]\n",
    "rez = []\n",
    "for word1 in c:\n",
    "    for word2 in d:\n",
    "        wordFromList1 = wn.synsets(word1)\n",
    "        wordFromList2 = wn.synsets(word2)\n",
    "        if wordFromList1 and wordFromList2: #Thanks to @alexis' note\n",
    "            s = wordFromList1[0].wup_similarity(wordFromList2[0])\n",
    "            rez.append(s)\n",
    "reduce(lambda x,y: x+y, rez)/len(rez)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "here we see the pitfall in relying on the similarity measure of the two doccuments as their scope differs(invest in shares vs invest in shares of **INDIA**). To extend this baseline model we will look at parts of speech and entity recognition to help parse scope.\n",
    "\n",
    "We can however use the similarity as the first filter to remove completely unrelated questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def wn_sim(q1doc,q2doc):\n",
    "    for word1 in q1doc:\n",
    "        max_w1 = 0.0\n",
    "        for word2 in q2doc:\n",
    "            wordFromList1 = wn.synsets(word1.text)\n",
    "            wordFromList2 = wn.synsets(word2.text)\n",
    "            if wordFromList1 and wordFromList2:\n",
    "                s = wordFromList1[0].wup_similarity(wordFromList2[0])\n",
    "                if (s > max_w1):\n",
    "                    max_w1 = s\n",
    "            rez.append(max_w1)\n",
    "    return reduce(lambda x,y: x+y, rez)/len(rez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "objts = []\n",
    "\n",
    "def parse_clean(row):\n",
    "    q1_txt = [word for word in row['question1'].split() if word not in stopwords.words('english')]\n",
    "    q2_txt = [word for word in row['question2'].split() if word not in stopwords.words('english')]\n",
    "    return (q1_txt,q2_txt)\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "def tokenize(sent):\n",
    "    tokens = word_tokenize(sent)\n",
    "    return tokens\n",
    "\n",
    "def parse_clean(row):\n",
    "    q1_txt = [word for word in row['question1'].split() if word not in stopwords.words('english')]\n",
    "    q2_txt = [word for word in row['question2'].split() if word not in stopwords.words('english')]\n",
    "    return (q1_txt,q2_txt)\n",
    "\n",
    "import nltk\n",
    "def pos_tag(tokens):\n",
    "    return nltk.pos_tag(tokens)\n",
    "\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def stem(tokens):\n",
    "    pstem = PorterStemmer()\n",
    "    return map(lambda x: pstem.stem(x),tokens)\n",
    "\n",
    "\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def word_stem(tokens):\n",
    "    rez = []\n",
    "    for tok in tokens:\n",
    "        tokdk = nlp(unicode(tok))\n",
    "        pos = \"\"\n",
    "        for x in tokdk:\n",
    "            pos = x.pos_\n",
    "        pstem = WordNetLemmatizer()\n",
    "        print pos\n",
    "        rez.append(pstem.lemmatize(tok))\n",
    "    return rez \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "I will investigate regression on the POS tagging and have one feature be lemmatized verb similarity and entity similarites with penalty for entities not encluded"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# append all text of the questions together for training\n",
    "texts = map(lambda x:  str(x), train.loc[:,'question1']) \\\n",
    "+ map(lambda x:  str(x), train.loc[:,'question2']) \\\n",
    "+ map(lambda x: str(x), test.loc[:,'question1']) \\\n",
    "+ map(lambda x: str(x), test.loc[:,'question2'])\n",
    "\n",
    "# delete duplicates\n",
    "qs = set(texts)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "qs = [x for x in qs if x is not None]\n",
    "    \n",
    "lemmas = []\n",
    "st_tm = time.time()\n",
    "for idx, doc in enumerate(docs):\n",
    "    lemma = []\n",
    "    for word in doc:\n",
    "        if not word.is_punct:\n",
    "            if word.lemma_ == '-PRON-':\n",
    "                lemma.append(unicode(word))\n",
    "            else:\n",
    "                lemma.append(word.lemma_)\n",
    "    lemmas.append(lemma)\n",
    "    if idx % 100000 == 0:\n",
    "        print time.time() - st_tm\n",
    "print lemmas[:3]    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## in a different file \"train_wrod2vec.py\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "from gensim.models import word2vec\n",
    "# PRE-PROCESS THE SENTENCES AND STEM / LEMMATIZE!!!!\n",
    "def pipe_clean(sent):\n",
    "    \n",
    "sentz = [pipe_clean(x) for x in qs]\n",
    "#train our word to vec model and get ready\n",
    "model = word2vec.Word2Vec(sentz, min_count=1)\n",
    "model.save(\"saved_vec_mod.kf\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"saved_vec_mod.kf\")\n",
    "\n",
    "def vec_diff(doc1,doc2):\n",
    "    a = doc1.vector\n",
    "    b = doc2.vector\n",
    "    return a - b\n",
    "    \n",
    "def wmdist(sent1,sent2):\n",
    "    \"\"\"This requires that the parameters @sent1 @sent2 are lists of strings ideally\n",
    "    lemmatized and cleaned sentences from above\"\"\"\n",
    "    return model.wmdistance(sent1, sent2)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.84825569e-01,  -1.45079494e-02,  -1.18102714e-01,\n",
       "         2.24120393e-02,   4.02033962e-02,   7.42071047e-02,\n",
       "        -5.29460944e-02,  -8.10047314e-02,   4.59638089e-02,\n",
       "         8.38956833e-02,  -9.13993120e-02,   1.98517311e-02,\n",
       "         1.57576144e-01,  -3.26649062e-02,   6.07710928e-02,\n",
       "         2.86095738e-02,  -3.31799015e-02,   1.80741787e-01,\n",
       "        -9.44152623e-02,   4.09685075e-03,  -1.10343508e-02,\n",
       "         6.92504421e-02,  -3.26047540e-02,   1.95322931e-02,\n",
       "         2.66033038e-03,  -7.41237476e-02,  -3.12730931e-02,\n",
       "        -1.13286287e-01,  -3.75003293e-02,   8.37820172e-02,\n",
       "         6.05418310e-02,   9.34930742e-02,   4.99187037e-02,\n",
       "         4.36130837e-02,   1.34942234e-02,  -2.58105472e-02,\n",
       "         5.46048619e-02,  -1.78474814e-01,  -1.36214584e-01,\n",
       "        -8.10364038e-02,  -9.23259743e-03,   1.03099123e-02,\n",
       "        -7.78148472e-02,  -1.07806772e-02,   6.82395548e-02,\n",
       "        -4.94703203e-02,   1.57432109e-02,   4.07735258e-02,\n",
       "        -3.39489877e-02,  -3.98074389e-02,  -3.70888412e-02,\n",
       "         2.61110552e-02,  -1.29330754e-02,  -6.04365021e-03,\n",
       "         2.82998234e-02,   4.16137166e-02,  -4.51757833e-02,\n",
       "        -2.62642801e-02,  -4.18769345e-02,  -3.33109982e-02,\n",
       "        -3.48847359e-02,  -1.55740783e-01,  -5.68171293e-02,\n",
       "         3.14413905e-02,   2.03778595e-02,  -6.56145588e-02,\n",
       "        -3.09704207e-02,   7.90088177e-02,  -5.65324686e-02,\n",
       "         8.77676606e-02,  -2.31575593e-02,   1.30725861e-01,\n",
       "        -4.96044606e-02,   2.80894861e-02,  -1.16695184e-02,\n",
       "         2.08104968e-01,  -1.11041516e-01,  -2.86037643e-02,\n",
       "        -4.43605520e-02,  -3.55443656e-02,  -3.76401991e-02,\n",
       "         6.46385737e-03,   1.55095458e-01,   2.32394040e-03,\n",
       "        -1.15142409e-02,  -4.19313721e-02,  -5.91448843e-02,\n",
       "         2.41715461e-02,   6.74478859e-02,  -1.07732534e-01,\n",
       "        -1.75480410e-01,  -3.49373557e-02,  -5.06705344e-02,\n",
       "         1.15484754e-02,   6.57416284e-02,   9.93360505e-02,\n",
       "        -8.28625262e-03,   7.85198808e-02,   1.72829125e-02,\n",
       "        -1.00181084e-02,  -2.57682800e-03,  -7.89144635e-03,\n",
       "         1.14904694e-01,   2.56625712e-02,   6.69209063e-02,\n",
       "        -2.53214836e-02,   1.71341106e-01,  -2.53171846e-03,\n",
       "         7.22941011e-02,  -1.26034945e-01,   1.93347558e-02,\n",
       "         7.96124637e-02,  -8.89718235e-02,   2.48224735e-02,\n",
       "         1.33492783e-01,  -1.36022754e-02,   4.54917699e-02,\n",
       "         6.69399872e-02,   1.65215489e-02,   8.58707204e-02,\n",
       "         7.11783245e-02,  -1.32953510e-01,  -3.80114242e-02,\n",
       "         1.30093068e-01,   2.25813687e-02,   1.39054619e-02,\n",
       "        -8.28217268e-02,  -1.33374974e-01,   6.54599965e-02,\n",
       "        -2.43089609e-02,  -5.39855696e-02,   8.07101205e-02,\n",
       "        -8.38489607e-02,   1.62497833e-02,   1.58555359e-02,\n",
       "         9.37146395e-02,   2.84653045e-02,   1.03343919e-01,\n",
       "         1.78846493e-02,  -6.34189099e-02,  -2.88753390e-01,\n",
       "        -3.28669623e-02,   3.94099355e-02,   9.18046832e-02,\n",
       "        -1.25376254e-01,   4.57182638e-02,  -1.17179997e-01,\n",
       "         5.17748892e-02,   8.98938105e-02,  -4.53208499e-02,\n",
       "        -8.25538784e-02,   2.22285017e-02,  -1.84406266e-02,\n",
       "         4.16808873e-02,  -1.67792365e-02,  -3.94308120e-02,\n",
       "        -4.53912877e-02,   1.39400959e-02,   4.92560603e-02,\n",
       "        -8.16001296e-02,  -1.10536806e-01,   1.03590995e-01,\n",
       "         7.05517083e-02,  -2.36653090e-02,  -1.57334954e-02,\n",
       "        -1.96664467e-01,  -8.58572870e-03,   3.79228853e-02,\n",
       "         1.45143241e-01,  -1.64256394e-02,   6.73211962e-02,\n",
       "        -6.43528253e-02,  -7.71376491e-03,  -7.42596015e-02,\n",
       "        -4.41274084e-02,   5.61295822e-03,  -4.39636670e-02,\n",
       "        -3.09651345e-02,  -9.51712653e-02,  -1.08933650e-01,\n",
       "        -2.80775875e-02,  -8.85955989e-02,  -6.64067715e-02,\n",
       "         2.43461132e-03,   3.42618264e-02,  -2.31333748e-02,\n",
       "        -1.93722770e-02,  -1.22504726e-01,  -1.17283180e-01,\n",
       "        -1.30377352e-01,  -1.27505422e-01,  -5.67667484e-02,\n",
       "        -6.09252527e-02,  -7.79574662e-02,   1.06071003e-01,\n",
       "        -4.00369689e-02,  -1.86316669e-03,  -1.77072495e-01,\n",
       "        -6.01420999e-02,  -1.29083842e-01,  -1.01267122e-01,\n",
       "        -1.71830654e-02,  -5.60952276e-02,  -1.42063014e-03,\n",
       "         1.11448467e-01,   1.08905897e-01,   1.22462139e-02,\n",
       "        -1.23313796e-02,  -4.65394966e-02,   5.99353164e-02,\n",
       "        -9.37172771e-02,   9.22424197e-02,  -3.11572477e-03,\n",
       "        -1.80026814e-02,  -5.41681424e-02,   1.25782400e-01,\n",
       "        -1.02581076e-01,   1.65291578e-02,   8.02896172e-02,\n",
       "        -9.00289789e-02,  -2.50968337e-02,  -5.82597889e-02,\n",
       "         7.55098462e-03,  -1.50453523e-02,  -8.52324963e-02,\n",
       "        -1.15362369e-02,   8.36628228e-02,   7.94429891e-03,\n",
       "        -1.51041634e-02,  -6.24504015e-02,   3.00327241e-02,\n",
       "        -4.14048955e-02,  -6.79170117e-02,  -2.90422142e-03,\n",
       "        -1.13497339e-01,  -4.26927358e-02,  -2.71002613e-02,\n",
       "         1.32789940e-01,  -6.18722290e-03,   9.84367132e-02,\n",
       "        -3.22579592e-02,   1.00809634e-01,   8.20136070e-03,\n",
       "         4.70202044e-03,  -6.58793002e-02,   4.04394567e-02,\n",
       "         9.54709649e-02,  -1.23296678e-03,   2.36514956e-02,\n",
       "         3.86846103e-02,   7.69650936e-03,   1.08927637e-02,\n",
       "         1.03496827e-01,   3.71001624e-02,   2.65128911e-05,\n",
       "         8.35555047e-02,   4.56429925e-03,   6.14706203e-02,\n",
       "        -4.27500159e-03,  -2.48421542e-02,  -3.57468426e-02,\n",
       "         4.71423790e-02,   1.47650272e-01,  -8.80646333e-03,\n",
       "         3.26591618e-02,  -5.29501066e-02,  -9.25419480e-03,\n",
       "        -3.26321870e-02,  -8.13980699e-02,  -3.52315083e-02,\n",
       "         8.49632993e-02,   2.73134671e-02,  -2.71450728e-02,\n",
       "         2.59861723e-02,   6.90481365e-02,  -3.85569632e-02,\n",
       "        -6.88688457e-03,   8.59671384e-02,  -3.49883512e-02,\n",
       "         9.45463777e-04,   4.77451086e-03,   1.49336457e-03,\n",
       "        -2.20708549e-03,  -9.16848481e-02,  -1.07126817e-01,\n",
       "        -4.78420779e-02,   4.79402691e-02,   3.70173156e-03,\n",
       "        -7.56597221e-02,   1.31301954e-02,   5.92896044e-02,\n",
       "         1.88297220e-02,  -4.79889587e-02,  -1.20842800e-01,\n",
       "        -7.03752041e-03,  -6.53039739e-02,   1.62936561e-02,\n",
       "        -4.49088141e-02,   2.36639380e-02,  -1.08414486e-01], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_diff(nlp(unicode(train.loc[2]['question1'])),nlp(unicode(train.loc[2]['question2'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipe' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e5a96fdc9df2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mwmdist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0municode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pipe' is not defined"
     ]
    }
   ],
   "source": [
    "wmdist(pipe(nlp(unicode(train.loc[2]['question1']))), pipe(nlp(unicode(train.loc[2]['question2']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-70383f33388d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_docs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_docs' is not defined"
     ]
    }
   ],
   "source": [
    "len(train_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'I', u'increase', u'speed', u'internet', u'connection', u'use', u'vpn']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def pipe(question):\n",
    "    \"\"\" NEEDS A SPACY DOC OF THE QUESTION\"\"\"\n",
    "    b = [x for x in question if not x.is_punct]\n",
    "    a = [x.lemma_ if not x.lemma_ == '-PRON-' else x.text for x in b ]\n",
    "    return [x for x in a if x not in stop]\n",
    "\n",
    "pipe(nlp(unicode(train.loc[2]['question1'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "I increase speed internet connection use vpn "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clean(doc):\n",
    "    b = [x for x in doc if not x.is_punct]\n",
    "    a = [x.lemma_ if not x.lemma_ == '-PRON-' else x.text for x in b ]\n",
    "    return nlp(u''.join([x + ' ' for x in a if x not in stop]))\n",
    "clean(nlp(unicode(train.loc[2]['question1'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "READ THE DATA and then train model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2017-05-14 15:27:25\n",
      "100000 2017-05-14 15:28:15\n",
      "200000 2017-05-14 15:28:59\n",
      "300000 2017-05-14 15:29:42\n",
      "400000 2017-05-14 15:30:26\n",
      "500000 2017-05-14 15:31:08\n",
      "600000 2017-05-14 15:31:51\n",
      "700000 2017-05-14 15:32:32\n",
      "800000 2017-05-14 15:33:15\n"
     ]
    }
   ],
   "source": [
    "test_loc = 'test_dump.bin'\n",
    "train_loc = 'train_dump.bin'\n",
    "\n",
    "\n",
    "from spacy.tokens.doc import Doc\n",
    "test_docs = []\n",
    "train_docs = []\n",
    "i = 0\n",
    "#print datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "with open(train_loc, 'rb') as file_:\n",
    "    for byte_string in Doc.read_bytes(file_):\n",
    "        if i%100000 == 0: print i, datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        train_docs.append(clean(Doc(nlp.vocab).from_bytes(byte_string)))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 15:34:00\n",
      "10000 15:34:10\n",
      "20000 15:34:21\n",
      "30000 15:34:32\n",
      "40000 15:34:42\n",
      "50000 15:34:54\n",
      "60000 15:35:05\n",
      "70000 15:35:15\n",
      "80000 15:35:26\n",
      "90000 15:35:37\n",
      "100000 15:35:47\n",
      "110000 15:35:58\n",
      "120000 15:36:08\n",
      "130000 15:36:19\n",
      "140000 15:36:30\n",
      "150000 15:36:43\n",
      "160000 15:36:54\n",
      "170000 15:37:05\n",
      "180000 15:37:15\n",
      "190000 15:37:26\n",
      "200000 15:37:37\n",
      "210000 15:37:47\n",
      "220000 15:37:58\n",
      "230000 15:38:08\n",
      "240000 15:38:19\n",
      "250000 15:38:30\n",
      "260000 15:38:40\n",
      "270000 15:38:54\n",
      "280000 15:39:04\n",
      "290000 15:39:16\n",
      "300000 15:39:26\n",
      "310000 15:39:37\n",
      "320000 15:39:48\n",
      "330000 15:39:59\n",
      "340000 15:40:10\n",
      "350000 15:40:20\n",
      "360000 15:40:31\n",
      "370000 15:40:42\n",
      "380000 15:40:53\n",
      "390000 15:41:05\n",
      "400000 15:41:16\n"
     ]
    }
   ],
   "source": [
    "d = []\n",
    "ct = 0\n",
    "for q1,q2 in [(train_docs[2*i],train_docs[2*i+1]) for i in range(len(train_docs)/2)]:\n",
    "    d.append({\n",
    "              'sysim': q1.similarity(q2), # spacy sim\n",
    "#              'netsim':  wn_sim(q1c,q2c), #float sim by wordnet\n",
    "              'wmd': wmdist([x.text for x in q1], [x.text for x in q2]), # returns float dist\n",
    "              'vectdiff': vec_diff(q1,q2) # 300 dim vect\n",
    "    })\n",
    "    if ct % 10000 == 0: print ct, datetime.now().strftime('%H:%M:%S')\n",
    "    ct = ct+1    \n",
    "features = pd.DataFrame(d)\n",
    "features.to_csv('features.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "source": [
    "i=0\n",
    "with open(test_loc, 'rb') as file_:\n",
    "    for byte_string in Doc.read_bytes(file_):\n",
    "        if i%1000000 == 0: print i, datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        test_docs.append(Doc(nlp.vocab).from_bytes(byte_string))\n",
    "        i += 1\n",
    "        \n",
    "#         if i == 10: break\n",
    "#print datetime.now().strftime('%Y-%m-%d %H:%M:%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we have all the SpaCy docs loaded in, we will generate features and run a Regression on them.\n",
    "The methods we will use are:\n",
    "- logistic regression\n",
    "- least squares\n",
    "- svm\n",
    "- SGD"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
