{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en') # this should take some time like 10s to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loc = 'test_dump.bin'\n",
    "train_loc = 'train_dump.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(doc):\n",
    "    b = [x for x in doc if not x.is_punct]\n",
    "    a = [x.lemma_ if not x.lemma_ == '-PRON-' else x.text for x in b ]\n",
    "    return nlp(u''.join([x + ' ' for x in a if x not in stop]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2017-05-14 22:33:39\n",
      "100000 2017-05-14 22:34:45\n",
      "200000 2017-05-14 22:36:09\n",
      "300000 2017-05-14 22:37:17\n",
      "400000 2017-05-14 22:38:21\n",
      "500000 2017-05-14 22:39:30\n",
      "600000 2017-05-14 22:40:40\n",
      "700000 2017-05-14 22:41:57\n",
      "800000 2017-05-14 22:43:02\n"
     ]
    }
   ],
   "source": [
    "from spacy.tokens.doc import Doc\n",
    "test_docs = []\n",
    "train_docs = []\n",
    "i = 0\n",
    "#print datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "with open(train_loc, 'rb') as file_:\n",
    "    for byte_string in Doc.read_bytes(file_):\n",
    "        if i%100000 == 0: print i, datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        train_docs.append(clean(Doc(nlp.vocab).from_bytes(byte_string)))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load model for WMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec.load(\"saved_vec_mod.kf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vec_diff(doc1,doc2):\n",
    "    a = doc1.vector\n",
    "    b = doc2.vector\n",
    "    return a - b\n",
    "\n",
    "def wmdist(sent1,sent2):\n",
    "    \"\"\"This requires that the parameters @sent1 @sent2 are lists of strings ideally\n",
    "    lemmatized and cleaned sentences from above\"\"\"\n",
    "    return model.wmdistance(sent1, sent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 23:06:52\n",
      "10000 23:07:33\n",
      "20000 23:07:58\n",
      "30000 23:08:23\n",
      "40000 23:08:39\n",
      "50000 23:08:57\n",
      "60000 23:09:13\n",
      "70000 23:09:29\n",
      "80000 23:09:44\n",
      "90000 23:10:35\n",
      "100000 23:10:55\n",
      "110000 23:11:18\n",
      "120000 23:11:35\n",
      "130000 23:12:16\n",
      "140000 23:12:34\n",
      "150000 23:12:52\n",
      "160000 23:13:15\n",
      "170000 23:13:37\n",
      "180000 23:13:59\n",
      "190000 23:14:19\n",
      "200000 23:15:36\n",
      "210000 23:16:00\n",
      "220000 23:16:19\n",
      "230000 23:16:36\n",
      "240000 23:16:52\n",
      "250000 23:17:09\n",
      "260000 23:17:27\n",
      "270000 23:17:43\n",
      "280000 23:17:58\n",
      "290000 23:18:16\n",
      "300000 23:18:36\n",
      "310000 23:18:54\n",
      "320000 23:19:12\n",
      "330000 23:20:22\n",
      "340000 23:20:39\n",
      "350000 23:20:59\n",
      "360000 23:21:18\n",
      "370000 23:21:36\n",
      "380000 23:21:54\n",
      "390000 23:22:14\n",
      "400000 23:22:29\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c323b726e3d9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mct\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10000\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%H:%M:%S'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mct\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "d = []\n",
    "ct = 0\n",
    "for q1,q2 in [(train_docs[2*i],train_docs[2*i+1]) for i in range(len(train_docs)/2)]:\n",
    "    d.append({\n",
    "              'sysim': q1.similarity(q2), # spacy sim\n",
    "#              'netsim':  wn_sim(q1c,q2c), #float sim by wordnet\n",
    "              'wmd': wmdist([x.text for x in q1], [x.text for x in q2]), # returns float dist\n",
    "              'vectdiff': vec_diff(q1,q2) # 300 dim vect\n",
    "    })\n",
    "    if ct % 10000 == 0: print ct, datetime.now().strftime('%H:%M:%S')\n",
    "    ct = ct+1    \n",
    "features = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.58744273093\n",
      "10.5397637518\n",
      "9.05921030816\n",
      "19.3803418145\n",
      "11.7262981678\n",
      "7.01693150069\n",
      "18.2013025203\n",
      "4.1587520855\n",
      "0.0\n",
      "7.4102006765\n"
     ]
    }
   ],
   "source": [
    "pos = []\n",
    "neg = []\n",
    "for idx, row in features.iterrows():\n",
    "#     if idx == 10: break\n",
    "    if row[''] row['wmd']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joke Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('../data/charliesub.csv', 'w') as f:\n",
    "    f.write(\"test_id,is_duplicate\\n\")\n",
    "    for i in range(2345796):\n",
    "        f.write(str(i)+\",.17426\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Understanding log loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.69314718056\n"
     ]
    }
   ],
   "source": [
    "def logloss(py,px):\n",
    "    return -py*np.log(px) - (1-py)*np.log(1-px)\n",
    "\n",
    "py = .5\n",
    "px = .5\n",
    "print logloss(px,py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.2876820724517809"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
