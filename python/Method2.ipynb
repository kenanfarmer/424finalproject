{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens.doc import Doc\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loc = 'test_dump.bin'\n",
    "train_loc = 'train_dump.bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 6) (2345796, 3)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.csv').fillna('')\n",
    "test = pd.read_csv('../data/test.csv').fillna('')\n",
    "print train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = list(train['is_duplicate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read training data\n",
    "Elements $(2n, 2n+1)$ are the questions for pair $n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-14 15:39:00\n",
      "0 2017-05-14 15:39:00\n",
      "10000 2017-05-14 15:39:03\n",
      "20000 2017-05-14 15:39:04\n",
      "30000 2017-05-14 15:39:05\n",
      "40000 2017-05-14 15:39:06\n",
      "50000 2017-05-14 15:39:07\n",
      "60000 2017-05-14 15:39:08\n",
      "70000 2017-05-14 15:39:09\n",
      "80000 2017-05-14 15:39:11\n",
      "90000 2017-05-14 15:39:13\n",
      "100000 2017-05-14 15:39:14\n",
      "110000 2017-05-14 15:39:15\n",
      "120000 2017-05-14 15:39:18\n",
      "130000 2017-05-14 15:39:19\n",
      "140000 2017-05-14 15:39:21\n",
      "150000 2017-05-14 15:39:22\n",
      "160000 2017-05-14 15:39:24\n",
      "170000 2017-05-14 15:39:24\n",
      "180000 2017-05-14 15:39:25\n",
      "190000 2017-05-14 15:39:26\n",
      "200000 2017-05-14 15:39:27\n",
      "210000 2017-05-14 15:39:28\n",
      "220000 2017-05-14 15:39:29\n",
      "230000 2017-05-14 15:39:31\n",
      "240000 2017-05-14 15:39:32\n",
      "250000 2017-05-14 15:39:33\n",
      "260000 2017-05-14 15:39:35\n",
      "270000 2017-05-14 15:39:36\n",
      "280000 2017-05-14 15:39:37\n",
      "290000 2017-05-14 15:39:38\n",
      "300000 2017-05-14 15:39:40\n",
      "310000 2017-05-14 15:39:41\n",
      "320000 2017-05-14 15:39:43\n",
      "330000 2017-05-14 15:39:44\n",
      "340000 2017-05-14 15:39:45\n",
      "350000 2017-05-14 15:39:46\n",
      "360000 2017-05-14 15:39:47\n",
      "370000 2017-05-14 15:39:48\n",
      "380000 2017-05-14 15:39:49\n",
      "390000 2017-05-14 15:39:51\n",
      "400000 2017-05-14 15:39:52\n",
      "410000 2017-05-14 15:39:54\n",
      "420000 2017-05-14 15:39:56\n",
      "430000 2017-05-14 15:39:57\n",
      "440000 2017-05-14 15:39:59\n",
      "450000 2017-05-14 15:40:01\n",
      "460000 2017-05-14 15:40:02\n",
      "470000 2017-05-14 15:40:03\n",
      "480000 2017-05-14 15:40:04\n",
      "490000 2017-05-14 15:40:06\n",
      "500000 2017-05-14 15:40:07\n",
      "510000 2017-05-14 15:40:08\n",
      "520000 2017-05-14 15:40:09\n",
      "530000 2017-05-14 15:40:11\n",
      "540000 2017-05-14 15:40:12\n",
      "550000 2017-05-14 15:40:13\n",
      "560000 2017-05-14 15:40:14\n",
      "570000 2017-05-14 15:40:16\n",
      "580000 2017-05-14 15:40:17\n",
      "590000 2017-05-14 15:40:18\n",
      "600000 2017-05-14 15:40:18\n",
      "610000 2017-05-14 15:40:19\n",
      "620000 2017-05-14 15:40:20\n",
      "630000 2017-05-14 15:40:21\n",
      "640000 2017-05-14 15:40:22\n",
      "650000 2017-05-14 15:40:23\n",
      "660000 2017-05-14 15:40:23\n",
      "670000 2017-05-14 15:40:25\n",
      "680000 2017-05-14 15:40:26\n",
      "690000 2017-05-14 15:40:27\n",
      "700000 2017-05-14 15:40:28\n",
      "710000 2017-05-14 15:40:29\n",
      "720000 2017-05-14 15:40:31\n",
      "730000 2017-05-14 15:40:32\n",
      "740000 2017-05-14 15:40:33\n",
      "750000 2017-05-14 15:40:34\n",
      "760000 2017-05-14 15:40:34\n",
      "770000 2017-05-14 15:40:35\n",
      "780000 2017-05-14 15:40:36\n",
      "790000 2017-05-14 15:40:37\n",
      "800000 2017-05-14 15:40:38\n",
      "2017-05-14 15:40:39\n",
      "<type 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "i = 0\n",
    "print datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "with open(train_loc, 'rb') as file_:\n",
    "    for byte_string in Doc.read_bytes(file_):\n",
    "        if i%10000 == 0: print i, datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        docs.append(Doc(nlp.vocab).from_bytes(byte_string))\n",
    "        i += 1\n",
    "#         if i == 10: break\n",
    "print datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(type(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DNS,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[5].ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def covertPropn(pos):\n",
    "    if pos == u'PROPN':\n",
    "        return u'NOUN'\n",
    "    else: return pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def countQuestionPairs(docs):\n",
    "    fvs = []\n",
    "    for i in range(0,len(docs),2):\n",
    "        if i % 100000 == 0: print i\n",
    "        fv = {}\n",
    "        d1 = docs[i]\n",
    "        d2 = docs[i+1]\n",
    "\n",
    "        s1 = set(list(str(e) for e in d1.ents))\n",
    "        s2 = set(list(str(e) for e in d2.ents))\n",
    "\n",
    "        sameents = len(s1 & s2)\n",
    "        diffents = len(s1 ^ s2)\n",
    "        fv['sameents'] = sameents\n",
    "        fv['diffents'] = diffents\n",
    "\n",
    "        s1 = set([(covertPropn(word.pos_), word.lemma_) for word in d1 if not word.is_stop])\n",
    "        s2 = set([(covertPropn(word.pos_), word.lemma_ ) for word in d2 if not word.is_stop])\n",
    "        diff = s1 ^ s2\n",
    "        same =  s1 & s2\n",
    "\n",
    "        samenum = len(same)\n",
    "        same = Counter([t[0] for t in same])\n",
    "        fv.update(same)\n",
    "        fv['samenum'] = samenum\n",
    "\n",
    "        diffnum = len(diff)\n",
    "        diff = Counter([t[0] + '_d' for t in diff])\n",
    "        fv['diffnum'] = diffnum\n",
    "        fv.update(diff)\n",
    "\n",
    "    #     print(d1, d2)\n",
    "        fvs.append(fv)\n",
    "    return fvs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n"
     ]
    }
   ],
   "source": [
    "fvs = countQuestionPairs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sameents': 0, u'NOUN': 4, 'samenum': 6, 'diffents': 0, 'diffnum': 1, u'PUNCT': 1, u'VERB': 1, u'NOUN_d': 1}\n",
      "What is the step by step guide to invest in share market in india? What is the step by step guide to invest in share market?\n",
      "{'sameents': 2, u'NOUN': 4, 'samenum': 8, u'ADJ_d': 1, u'VERB_d': 2, 'diffents': 1, 'diffnum': 5, u'PUNCT': 4, u'NOUN_d': 2}\n",
      "What is the story of Kohinoor (Koh-i-Noor) Diamond? What would happen if the Indian government stole the Kohinoor (Koh-i-Noor) diamond back?\n",
      "{'sameents': 0, u'PRON_d': 1, u'NOUN': 1, 'samenum': 3, u'VERB_d': 1, 'diffents': 2, 'diffnum': 6, u'PUNCT': 1, u'VERB': 1, u'NOUN_d': 4}\n",
      "How can I increase the speed of my internet connection while using a VPN? How can Internet speed be increased by hacking through DNS?\n",
      "{'sameents': 0, u'ADV_d': 1, 'samenum': 1, u'ADJ_d': 1, u'VERB_d': 3, u'PUNCT_d': 2, u'NUM_d': 1, 'diffents': 1, 'diffnum': 10, u'PUNCT': 1, u'NOUN_d': 2}\n",
      "Why am I mentally very lonely? How can I solve it? Find the remainder when [math]23^{24}[/math] is divided by 24,23?\n",
      "{'sameents': 0, u'X_d': 1, u'NOUN': 2, 'samenum': 3, u'ADJ_d': 1, u'PUNCT_d': 1, u'VERB_d': 2, 'diffents': 0, 'diffnum': 10, u'PUNCT': 1, u'NOUN_d': 5}\n",
      "Which one dissolve in water quikly sugar, salt, methane and carbon di oxide? Which fish would survive in salt water?\n",
      "{'sameents': 0, u'NOUN': 3, 'samenum': 4, u'ADJ_d': 2, u'PUNCT_d': 5, u'VERB_d': 2, 'diffents': 4, 'diffnum': 11, u'PUNCT': 1, u'NOUN_d': 2}\n",
      "Astrology: I am a Capricorn Sun Cap moon and cap rising...what does that say about me? I'm a triple Capricorn (Sun, Moon and ascendant in Capricorn) What does this say about me?\n",
      "{'sameents': 0, u'ADV_d': 2, 'samenum': 1, u'ADJ_d': 2, u'VERB_d': 2, 'diffents': 0, 'diffnum': 9, u'PUNCT': 1, u'NOUN_d': 3}\n",
      "Should I buy tiago? What keeps childern active and far from phone and video games?\n",
      "{'sameents': 0, u'NOUN': 1, 'samenum': 2, u'ADJ_d': 2, 'diffents': 0, 'diffnum': 2, u'PUNCT': 1}\n",
      "How can I be a good geologist? What should I do to be a great geologist?\n",
      "{'sameents': 0, u'ADV': 1, u'PRON_d': 1, 'samenum': 3, u'PUNCT_d': 1, 'diffents': 0, 'diffnum': 4, u'PUNCT': 1, u'VERB': 1, u'CCONJ_d': 1, u'NOUN_d': 1}\n",
      "When do you use シ instead of し? When do you use \"&\" instead of \"and\"?\n",
      "{'sameents': 1, u'NOUN': 2, 'samenum': 4, u'ADJ_d': 1, u'PUNCT_d': 3, 'diffents': 1, 'diffnum': 8, u'PUNCT': 1, u'VERB': 1, u'NOUN_d': 4}\n",
      "Motorola (company): Can I hack my Charter Motorolla DCX3400? How do I hack Motorola DCX3400 for free internet?\n"
     ]
    }
   ],
   "source": [
    "for i, f in enumerate(fvs[:10]):\n",
    "    print f\n",
    "    print docs[2 *i], docs[2 *i + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\n",
    "v = DictVectorizer(sparse=False)\n",
    "X = v.fit_transform(fvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=20,\n",
      "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=False)\n",
      "Log Loss: -0.49835\n",
      "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)\n",
      "Log Loss: -0.96641\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Log Loss: -0.55215\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Log Loss: -0.54342\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 20\n",
    "clfs = [RandomForestClassifier(min_samples_leaf=20, min_samples_split=10, n_jobs=-1), MultinomialNB(), SGDClassifier(loss='log'), LogisticRegression()]\n",
    "# clfs = [SVC(kernel='linear', probability=True)] \n",
    "\n",
    "for clf in clfs:\n",
    "    print clf\n",
    "    scores = cross_val_score(clf, X, target, cv=3, scoring='neg_log_loss')\n",
    "    print \"Log Loss: %0.5f\" % scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=7,\n",
      "            min_samples_split=30, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=50, n_jobs=-1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=True)\n",
      "Log Loss: -0.49019\n"
     ]
    }
   ],
   "source": [
    "n_estimators = 20\n",
    "clfs = [RandomForestClassifier(n_estimators= 50, min_samples_leaf=7, min_samples_split=30, n_jobs=-1, warm_start=True)]\n",
    "# clfs = [SVC(kernel='linear', probability=True)] \n",
    "\n",
    "for clf in clfs:\n",
    "    print clf\n",
    "    scores = cross_val_score(clf, X, target, cv=3, scoring='neg_log_loss')\n",
    "    print \"Log Loss: %0.5f\" % scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read test questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_docs = []\n",
    "i = 0\n",
    "with open(test_loc, 'rb') as file_:\n",
    "    for byte_string in Doc.read_bytes(file_):\n",
    "        test_docs.append(Doc(nlp.vocab).from_bytes(byte_string))\n",
    "        i += 1\n",
    "print(type(test_docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_docs[1345288]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_fvs = countQuestionPairs(test_docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
