{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "### Import precomp libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en') # this should take some time like 10s to load\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_loc = 'test_dump.bin'\n",
    "train_loc = 'train_dump.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from spacy.tokens.doc import Doc\n",
    "import pickle \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.ensemble import RandomForestClassifier, BaggingClassifier\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(404290, 6) (2345796, 3)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "print train.shape, test.shape\n",
    "target = train['is_duplicate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-05-14 16:01:18\n",
      "0 2017-05-14 16:01:18\n",
      "10000 2017-05-14 16:01:21\n",
      "20000 2017-05-14 16:01:22\n",
      "30000 2017-05-14 16:01:23\n",
      "40000 2017-05-14 16:01:24\n",
      "50000 2017-05-14 16:01:25\n",
      "60000 2017-05-14 16:01:26\n",
      "70000 2017-05-14 16:01:27\n",
      "80000 2017-05-14 16:01:28\n",
      "90000 2017-05-14 16:01:29\n",
      "100000 2017-05-14 16:01:30\n",
      "110000 2017-05-14 16:01:31\n",
      "120000 2017-05-14 16:01:32\n",
      "130000 2017-05-14 16:01:33\n",
      "140000 2017-05-14 16:01:34\n",
      "150000 2017-05-14 16:01:35\n",
      "160000 2017-05-14 16:01:35\n",
      "170000 2017-05-14 16:01:36\n",
      "180000 2017-05-14 16:01:37\n",
      "190000 2017-05-14 16:01:38\n",
      "200000 2017-05-14 16:01:39\n",
      "210000 2017-05-14 16:01:40\n",
      "220000 2017-05-14 16:01:41\n",
      "230000 2017-05-14 16:01:42\n",
      "240000 2017-05-14 16:01:43\n",
      "250000 2017-05-14 16:01:44\n",
      "260000 2017-05-14 16:01:45\n",
      "270000 2017-05-14 16:01:45\n",
      "280000 2017-05-14 16:01:46\n",
      "290000 2017-05-14 16:01:47\n",
      "300000 2017-05-14 16:01:48\n",
      "310000 2017-05-14 16:01:49\n",
      "320000 2017-05-14 16:01:50\n",
      "330000 2017-05-14 16:01:51\n",
      "340000 2017-05-14 16:01:52\n",
      "350000 2017-05-14 16:01:53\n",
      "360000 2017-05-14 16:01:54\n",
      "370000 2017-05-14 16:01:55\n",
      "380000 2017-05-14 16:01:56\n",
      "390000 2017-05-14 16:01:56\n",
      "400000 2017-05-14 16:01:57\n",
      "410000 2017-05-14 16:01:58\n",
      "420000 2017-05-14 16:02:00\n",
      "430000 2017-05-14 16:02:01\n",
      "440000 2017-05-14 16:02:02\n",
      "450000 2017-05-14 16:02:03\n",
      "460000 2017-05-14 16:02:04\n",
      "470000 2017-05-14 16:02:05\n",
      "480000 2017-05-14 16:02:06\n",
      "490000 2017-05-14 16:02:06\n",
      "500000 2017-05-14 16:02:07\n",
      "510000 2017-05-14 16:02:08\n",
      "520000 2017-05-14 16:02:10\n",
      "530000 2017-05-14 16:02:11\n",
      "540000 2017-05-14 16:02:12\n",
      "550000 2017-05-14 16:02:13\n",
      "560000 2017-05-14 16:02:13\n",
      "570000 2017-05-14 16:02:14\n",
      "580000 2017-05-14 16:02:15\n",
      "590000 2017-05-14 16:02:16\n",
      "600000 2017-05-14 16:02:18\n",
      "610000 2017-05-14 16:02:19\n",
      "620000 2017-05-14 16:02:20\n",
      "630000 2017-05-14 16:02:21\n",
      "640000 2017-05-14 16:02:22\n",
      "650000 2017-05-14 16:02:23\n",
      "660000 2017-05-14 16:02:24\n",
      "670000 2017-05-14 16:02:25\n",
      "680000 2017-05-14 16:02:26\n",
      "690000 2017-05-14 16:02:27\n",
      "700000 2017-05-14 16:02:27\n",
      "710000 2017-05-14 16:02:28\n",
      "720000 2017-05-14 16:02:29\n",
      "730000 2017-05-14 16:02:30\n",
      "740000 2017-05-14 16:02:30\n",
      "750000 2017-05-14 16:02:31\n",
      "760000 2017-05-14 16:02:32\n",
      "770000 2017-05-14 16:02:33\n",
      "780000 2017-05-14 16:02:33\n",
      "790000 2017-05-14 16:02:34\n",
      "800000 2017-05-14 16:02:35\n",
      "2017-05-14 16:02:36\n",
      "<type 'spacy.tokens.doc.Doc'>\n"
     ]
    }
   ],
   "source": [
    "docs = []\n",
    "i = 0\n",
    "print datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "with open(train_loc, 'rb') as file_:\n",
    "    for byte_string in Doc.read_bytes(file_):\n",
    "        if i%10000 == 0: print i, datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "        docs.append(Doc(nlp.vocab).from_bytes(byte_string))\n",
    "        i += 1\n",
    "#         if i == 10: break\n",
    "print datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "print(type(docs[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def wordPairs(docs):\n",
    "    fvs = []\n",
    "    for i in range(0,len(docs),2):\n",
    "        if i % 100000 == 0: print i\n",
    "        fv = {}\n",
    "        d1 = docs[i]\n",
    "        d2 = docs[i+1]\n",
    "        \n",
    "        s1 = set([word.lemma_ for word in d1 if not word.is_stop and word.pos is not 95])\n",
    "        s2 = set([word.lemma_  for word in d2 if not word.is_stop and word.pos is not 95])\n",
    "        diff = s1 ^ s2\n",
    "        same =  s1 & s2\n",
    "\n",
    "        same = Counter([w for w in same])\n",
    "        fv.update(same)\n",
    "\n",
    "        diff = Counter([w + '_d' for w in diff])\n",
    "        fv.update(diff)\n",
    "\n",
    "        fvs.append(fv)\n",
    "        \n",
    "    return fvs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n",
      "300000\n",
      "400000\n",
      "500000\n",
      "600000\n",
      "700000\n",
      "800000\n"
     ]
    }
   ],
   "source": [
    "fvs = wordPairs(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "v = DictVectorizer()\n",
    "t = TfidfTransformer(smooth_idf=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = v.fit_transform(fvs)\n",
    "# X = t.fit_transform(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404290, 112503)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log Loss: -0.41635\n"
     ]
    }
   ],
   "source": [
    "# Training classifiers\n",
    "\n",
    "clf1 = MultinomialNB()\n",
    "clf2 = SGDClassifier(loss='log')\n",
    "clf3 = LogisticRegression()\n",
    "eclf = VotingClassifier(estimators=[('mn', clf1), ('sgd', clf2), ('lr', clf3)], voting='soft', weights=[1,1,4])\n",
    "\n",
    "\n",
    "scores = cross_val_score(eclf, X, target, cv=3, scoring='neg_log_loss')\n",
    "print \"Log Loss: %0.5f\" % scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB(alpha=1, class_prior=None, fit_prior=True)\n",
      "Log Loss: -0.51773\n",
      "SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='log', n_iter=5, n_jobs=1,\n",
      "       penalty='l2', power_t=0.5, random_state=None, shuffle=True,\n",
      "       verbose=0, warm_start=False)\n",
      "Log Loss: -0.46116\n",
      "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)\n",
      "Log Loss: -0.41490\n",
      "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=50, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_split=1e-07, min_samples_leaf=8,\n",
      "            min_samples_split=30, min_weight_fraction_leaf=0.0,\n",
      "            n_estimators=10, n_jobs=-1, oob_score=False, random_state=None,\n",
      "            verbose=0, warm_start=True)\n",
      "Log Loss: -0.61616\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "clfs = [MultinomialNB(alpha=1), SGDClassifier(loss='log'), LogisticRegression(), RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=50, min_samples_split=30, min_samples_leaf=8, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_split=1e-07, bootstrap=True, oob_score=False, n_jobs=-1, warm_start=True, class_weight=None)]\n",
    "\n",
    "for clf in clfs:\n",
    "    print clf\n",
    "    scores = cross_val_score(clf, X, target, cv=3, scoring='neg_log_loss')\n",
    "    print \"Log Loss: %0.5f\" % scores.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_docs = []\n",
    "i = 0\n",
    "with open(test_loc, 'rb') as file_:\n",
    "    for byte_string in Doc.read_bytes(file_):\n",
    "        test_docs.append(Doc(nlp.vocab).from_bytes(byte_string))\n",
    "        i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('submission', 'w')\n",
    "f.write('test_id,is_duplicate\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fv_test = wordPairs(test_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fv_test = v.transform(fv_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = LogisticRegression().fit(X, target)\n",
    "prob = clf.predict_proba(fv_test)\n",
    "for i, p in enumerate(prob):\n",
    "    f.write(str(i) + ',' + str(p[0]) + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
