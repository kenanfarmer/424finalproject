{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "### Import precomp libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The line_profiler extension is already loaded. To reload it, use:\n",
      "  %reload_ext line_profiler\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en') # this should take some time like 10s to load\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import the spacy english model\n",
    "# \"python -m spacy download en\"   to download english models\n",
    "\n",
    "import pickle \n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROJECT structure: data directory with the two csvs of train,test and the ipython notbook in its own directory\n",
    "# loading the data\n",
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "print train.shape, test.shape\n",
    "train[379205]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "def stem \n",
    "    porter = PorterStemmer()\n",
    "    for w in l:\n",
    "        porter.stem(w)\n",
    "stem('What are the stories of Kohinoor (Koh-i-Noor) Diamond? men walked to the stores to buy shirts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({u'(': 1,\n",
       "         u')': 1,\n",
       "         u'-': 2,\n",
       "         u'?': 1,\n",
       "         u'buy': 1,\n",
       "         u'diamond': 1,\n",
       "         u'koh': 1,\n",
       "         u'kohinoor': 1,\n",
       "         u'man': 1,\n",
       "         u'noor': 1,\n",
       "         u'shirt': 1,\n",
       "         u'store': 1,\n",
       "         u'story': 1,\n",
       "         u'walk': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def parse(sentence):\n",
    "    bow = []\n",
    "    sentence = sentence.decode('utf-8')\n",
    "    p = nlp(sentence)\n",
    "    for word in p:\n",
    "        if word.is_stop: continue\n",
    "        bow.append(word.lemma_)\n",
    "    return Counter(bow)\n",
    "        \n",
    "parse('What are the stories of Kohinoor (Koh-i-Noor) Diamond? men walked to the stores to buy shirts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({u'india': 1, u'invest': 0, u'step': 0, u'?': 0, u'share': 0, u'guide': 0, u'market': 0})\n",
      "Counter({u'story': 1, u'koh': 0, u'diamond': 0, u')': 0, u'(': 0, u'-': 0, u'kohinoor': 0, u'noor': 0, u'?': 0, u'government': -1, u'indian': -1, u'happen': -1, u'steal': -1})\n",
      "Counter({u'connection': 1, u'vpn': 1, u'increase': 0, u'internet': 0, u'speed': 0, u'?': 0, u'hack': -1, u'dns': -1})\n",
      "Counter({u'lonely': 1, u'solve': 1, u'mentally': 1, u'?': 1, u'divide': -1, u']': -1, u'math]23^{24}[/math': -1, u'24,23': -1, u'[': -1, u'remainder': -1, u'find': -1})\n",
      "Counter({u',': 2, u'di': 1, u'sugar': 1, u'oxide': 1, u'carbon': 1, u'quikly': 1, u'dissolve': 1, u'methane': 1, u'water': 0, u'salt': 0, u'?': 0, u'survive': -1, u'fish': -1})\n",
      "Counter({u'cap': 2, u'...': 1, u'rise': 1, u':': 1, u'astrology': 1, u'sun': 0, u'moon': 0, u'?': 0, u'ascendant': -1, u'be': -1, u'capricorn': -1, u',': -1, u')': -1, u'triple': -1, u'(': -1})\n",
      "Counter({u'buy': 1, u'tiago': 1, u'?': 0, u'childern': -1, u'far': -1, u'keep': -1, u'phone': -1, u'game': -1, u'video': -1, u'active': -1})\n",
      "Counter({u'good': 1, u'geologist': 0, u'?': 0, u'great': -1})\n",
      "Counter({u'\\u30b7': 1, u'\\u3057': 1, u'use': 0, u'instead': 0, u'?': 0, u'&': -1, u'\"': -4})\n",
      "Counter({u'charter': 1, u')': 1, u'(': 1, u'company': 1, u'motorolla': 1, u':': 1, u'motorola': 0, u'dcx3400': 0, u'hack': 0, u'?': 0, u'internet': -1, u'free': -1})\n",
      "Counter({u'biprism': 1, u'fresnel': 1, u'find': 1, u'slit': 1, u'separation': 1, u'method': 1, u'?': 0, u'durability': -1, u'component': -1, u'laptops': -1, u'thing': -1, u'reliability': -1, u'tell': -1, u'technician': -1})\n"
     ]
    }
   ],
   "source": [
    "def print_vecs(corpus):\n",
    "    vect_list = []\n",
    "    results = []\n",
    "    n = len(train)\n",
    "    for i in range(n): \n",
    "        if i > 10: return \n",
    "        f = train.loc[i]\n",
    "        q1, q2 = f['question1'], f['question2']\n",
    "        first = parse(q1)\n",
    "        first.subtract(parse(q2))\n",
    "        print first\n",
    "print_vecs(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_vecs(corpus, results=False):\n",
    "    vect_list = []\n",
    "    results = []\n",
    "    n = len(corpus)\n",
    "    for i in range(n): \n",
    "        if i % 10000 == 0: print i\n",
    "        f = corpus.loc[i]\n",
    "        q1, q2 = f['question1'], f['question2']\n",
    "        try: \n",
    "            p1 = parse(q1)\n",
    "            p2 = parse(q2)\n",
    "            diff1 = p1 - p2\n",
    "            diff2 = p2 - p1\n",
    "            vect_list.append((diff1, diff2))\n",
    "            if results: results.append(f['is_duplicate'])\n",
    "        except Exception as e:\n",
    "            vect_list.append('')\n",
    "            print(i, e)\n",
    "    return fv, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-e2165b2cec76>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfv_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_vecs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-077ccc25725d>\u001b[0m in \u001b[0;36mmake_vecs\u001b[0;34m(corpus, results)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mq1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'question2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mp1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mdiff1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mp2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-3f5fa2832cc1>\u001b[0m in \u001b[0;36mparse\u001b[0;34m(sentence)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msentence\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msentence\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_stop\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spacy/language.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, tag, parse, entity)\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0;34m'An'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'NN'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m         \"\"\"\n\u001b[0;32m--> 320\u001b[0;31m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mentity\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mentity\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m             \u001b[0;31m# Add any of the entity labels already set, in case we don't have them.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spacy/language.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m    291\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'create_make_doc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'make_doc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'pipeline'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverrides\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pipeline'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer.__call__ (spacy/tokenizer.cpp:5324)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    150\u001b[0m                     \u001b[0mcache_hit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcache_hit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m                     \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspacy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._tokenize (spacy/tokenizer.cpp:6060)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0morig_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mspan\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_affixes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0mprefixes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_attach_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0mprefixes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m&\u001b[0m\u001b[0msuffixes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_save_cached\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m&\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0morig_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morig_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0morig_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spacy/tokenizer.pyx\u001b[0m in \u001b[0;36mspacy.tokenizer.Tokenizer._attach_tokens (spacy/tokenizer.cpp:7129)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    277\u001b[0m                 \u001b[0mmatches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_infix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatches\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m                     \u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpush_back\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m                     \u001b[0;31m# let's say we have dyn-o-mite-dave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab.get (spacy/vocab.cpp:6986)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mlex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_lexeme\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0mcdef\u001b[0m \u001b[0mconst\u001b[0m \u001b[0mLexemeC\u001b[0m\u001b[0;34m*\u001b[0m \u001b[0mget_by_orth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPool\u001b[0m \u001b[0mmem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattr_t\u001b[0m \u001b[0morth\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mNULL\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spacy/vocab.pyx\u001b[0m in \u001b[0;36mspacy.vocab.Vocab._new_lexeme (spacy/vocab.cpp:7439)\u001b[0;34m()\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlex_attr_getters\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlex_attr_getters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 275\u001b[0;31m                 \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    276\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m                     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/spacy/language.pyc\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(string)\u001b[0m\n\u001b[1;32m    153\u001b[0m     lex_attr_getters = {\n\u001b[1;32m    154\u001b[0m         \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLOWER\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m         \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNORM\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m         \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSHAPE\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0morth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[0mattrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREFIX\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "fv_train, target = make_vecs(train, results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n"
     ]
    }
   ],
   "source": [
    "fv_test, results = make_vecs(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# with open('fv_train','w') as f:\n",
    "#     pickle.dump(fv_train_,f)\n",
    "# with open('target','w') as f:\n",
    "#     pickle.dump(target,f)\n",
    "with open('fv_test','w') as f:\n",
    "    pickle.dump(fv_test,f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load from pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fv_train_p = pickle.load(open('fv_train', 'rb'))\n",
    "fv_test_p = pickle.load(open('fv_test', 'rb'))\n",
    "target_p = pickle.load(open('target', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Counter({u'slit': 1, u'separation': 1, u'biprism': 1, u'fresnel': 1, u'method': 1, u'find': 1}), Counter({u'laptops': 1, u'thing': 1, u'reliability': 1, u'durability': 1, u'technician': 1, u'component': 1, u'tell': 1}))\n",
      "(Counter(), Counter({u'happen': 1, u'indian': 1, u'steal': 1, u'government': 1}), Counter({u'dns': 1, u'hack': 1}), Counter({u'math]23^{24}[/math': 1, u'24,23': 1, u'divide': 1, u'remainder': 1, u'[': 1, u']': 1, u'find': 1}), Counter({u'survive': 1, u'fish': 1}), Counter({u'be': 1, u'ascendant': 1, u'triple': 1, u')': 1, u'(': 1, u',': 1, u'capricorn': 1}), Counter({u'phone': 1, u'game': 1, u'video': 1, u'childern': 1, u'far': 1, u'active': 1, u'keep': 1}), Counter({u'great': 1}), Counter({u'\"': 4, u'&': 1}), Counter({u'free': 1, u'internet': 1}))\n"
     ]
    }
   ],
   "source": [
    "print(fv_train_p[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(vectors):\n",
    "    v1, v2 = zip(*vect_list_p)\n",
    "    vectorizer = DictVectorizer()\n",
    "    x1 = vectorizer.fit_transform(v1)\n",
    "    x2 = vectorizer.fit_transform(v2)\n",
    "    X = hstack([x1,x2])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = vectorize(fv_train_p)\n",
    "target = results_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(404288, 116563)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.68 (+/- 0.00)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.model_selection import cross_val_score\n",
    "clf = MultinomialNB()\n",
    "scores = cross_val_score(clf, X, target, cv=5)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = open('submission', 'rb'))\n",
    "f.write('test_id,is_duplicate')\n",
    "fv_test = vectorize(fv_test_p)\n",
    "clf = MultinomialNB().fit(X, target)\n",
    "predict_proba(fv_test_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this procedure will be mapped to each row\n",
    "def eval_sysim(row):\n",
    "    q1_txt = nlp(unicode(row['question1']))\n",
    "    q2_txt = nlp(unicode(row['question2']))\n",
    "\n",
    "    # feature: similarity measure from built-in spacy\n",
    "    #word net similarity? or hamming distance of the strings?\n",
    "    sy_sim = q1_txt.similarity(q2_txt)\n",
    "\n",
    "    #feature: \n",
    "    return sy_sim\n",
    "    \n",
    "#unit test(s)\n",
    "utest = train.loc[1]\n",
    "print eval_sysim(utest)\n",
    "utest = train.loc[3]\n",
    "print eval_sysim(utest)\n",
    "utest = train.loc[9]\n",
    "print eval_sysim(utest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here we see the pitfall in relying on the similarity measure of the two doccuments as their scope differs(invest in shares vs invest in shares of **INDIA**). To extend this baseline model we will look at parts of speech and entity recognition to help parse scope.\n",
    "\n",
    "We can however use the similarity as the first filter to remove completely unrelated questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
